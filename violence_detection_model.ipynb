{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "violence_detection_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1BCrWWiIFbEP8CNfnVoBUmeqDTai0DcK5",
      "authorship_tag": "ABX9TyM1ruFxpIz9izEzN3PX/e0C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhohye22/bigdata_project/blob/main/violence_detection_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU1G6QL6wYDR"
      },
      "source": [
        "import cv2  \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os \n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmnT-G18zTyz"
      },
      "source": [
        "# 새롭게 추가된 영상이 있는 폴더\n",
        "new_video_folder_dir = '....' \n",
        "\n",
        "# 새 영상에서 변환한 npy파일이 있는 폴더\n",
        "new_npy_dir = '...'\n",
        "\n",
        "# npy 파일이 있는 폴더의 상위폴더\n",
        "new_npy_ParentFolder= '...'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GuMMznyDqMP"
      },
      "source": [
        "## load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tc8nNjz3w51"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXgqZszbJ3Nt"
      },
      "source": [
        "from keras.models import load_model\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "model = load_model('/content/drive/MyDrive/4_19data/keras_model.h5',\n",
        "                  compile=False)\n",
        "model.compile(optimizer=sgd,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fscpavXywy-i"
      },
      "source": [
        "# Transform Video to .npy Format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkAz5Kqxw1ba"
      },
      "source": [
        "\n",
        "def getOpticalFlow(video):\n",
        "    \"\"\"Calculate dense optical flow of input video\n",
        "    Args:\n",
        "        video: the input video with shape of [frames,height,width,channel]. dtype=np.array\n",
        "    Returns:\n",
        "        flows_x: the optical flow at x-axis, with the shape of [frames,height,width,channel]\n",
        "        flows_y: the optical flow at y-axis, with the shape of [frames,height,width,channel]\n",
        "    \"\"\"\n",
        "    # initialize the list of optical flows\n",
        "    gray_video = []\n",
        "    for i in range(len(video)):\n",
        "        img = cv2.cvtColor(video[i], cv2.COLOR_RGB2GRAY)\n",
        "        gray_video.append(np.reshape(img,(224,224,1)))\n",
        "\n",
        "    flows = []\n",
        "    for i in range(0,len(video)-1):\n",
        "        # calculate optical flow between each pair of frames\n",
        "        flow = cv2.calcOpticalFlowFarneback(gray_video[i], gray_video[i+1], None, 0.5, 3, 15, 3, 5, 1.2, cv2.OPTFLOW_FARNEBACK_GAUSSIAN)\n",
        "        # subtract the mean in order to eliminate the movement of camera\n",
        "        flow[..., 0] -= np.mean(flow[..., 0])\n",
        "        flow[..., 1] -= np.mean(flow[..., 1])\n",
        "        # normalize each component in optical flow\n",
        "        flow[..., 0] = cv2.normalize(flow[..., 0],None,0,255,cv2.NORM_MINMAX)\n",
        "        flow[..., 1] = cv2.normalize(flow[..., 1],None,0,255,cv2.NORM_MINMAX)\n",
        "        # Add into list \n",
        "        flows.append(flow)\n",
        "        \n",
        "    # Padding the last frame as empty array\n",
        "    flows.append(np.zeros((224,224,2)))\n",
        "      \n",
        "    return np.array(flows, dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DTGPB3Byhfb"
      },
      "source": [
        "## 영상 > npy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JERpT7pw_lq"
      },
      "source": [
        "def Video2Npy(file_path, resize=(224,224)):\n",
        "    \"\"\"Load video and tansfer it into .npy format\n",
        "    Args:\n",
        "        file_path: the path of video file\n",
        "        resize: the target resolution of output video\n",
        "    Returns:\n",
        "        frames: gray-scale video\n",
        "        flows: magnitude video of optical flows \n",
        "    \"\"\"\n",
        "    # Load video\n",
        "    cap = cv2.VideoCapture(file_path)\n",
        "    # Get number of frames\n",
        "    len_frames = int(cap.get(7))\n",
        "    # Extract frames from video\n",
        "    try:\n",
        "        frames = []\n",
        "        for i in range(len_frames-1):\n",
        "            _, frame = cap.read()\n",
        "            frame = cv2.resize(frame,resize, interpolation=cv2.INTER_AREA)\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = np.reshape(frame, (224,224,3))\n",
        "            frames.append(frame)   \n",
        "    except:\n",
        "        print(\"Error: \", file_path, len_frames,i)\n",
        "    finally:\n",
        "        frames = np.array(frames)\n",
        "        cap.release()\n",
        "            \n",
        "    # Get the optical flow of video\n",
        "    flows = getOpticalFlow(frames)\n",
        "    \n",
        "    result = np.zeros((len(flows),224,224,5))\n",
        "    result[...,:3] = frames\n",
        "    result[...,3:] = flows\n",
        "    \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfIFJ4W4QPjo"
      },
      "source": [
        "# TO DO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-xTNicVyp9D"
      },
      "source": [
        "## 폴더 속 최근에 추가된 영상의 npy 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C37WgJlQxAfS"
      },
      "source": [
        "def RecentVideoSave2Npy(file_dir, save_dir):\n",
        "    \"\"\"Transfer the video and save them into specified directory\n",
        "    Args:\n",
        "        file_dir: 추가된 영상이 있는 폴더\n",
        "        save_dir: npy로 변환후 저장될 위치\n",
        "    \"\"\"\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    # List the files\n",
        "    videos = os.listdir(file_dir)\n",
        "    \n",
        "    # 폴더에 최근에 추가된 영상 이름(v) 가져오기\n",
        "    \n",
        "    files_dir = \"/content/drive/MyDrive/4_19data/example/\" # 파일들이 들어있는 폴더\n",
        "    file_name_and_time_lst = []\n",
        "    # 해당 경로에 있는 파일들의 생성시간을 함께 리스트로 넣어줌. \n",
        "    for f_name in os.listdir(f\"{files_dir}\"):\n",
        "        written_time = os.path.getctime(f\"{files_dir}{f_name}\")\n",
        "        file_name_and_time_lst.append((f_name, written_time))\n",
        "    # 생성시간 역순으로 정렬하고, \n",
        "    sorted_file_lst = sorted(file_name_and_time_lst, key=lambda x: x[1], reverse=True)\n",
        "    # 가장 앞에 있는 놈을 넣어준다.\n",
        "    recent_file = sorted_file_lst[0]\n",
        "    recent_file_name = recent_file[0]\n",
        "    v = recent_file_name\n",
        "    \n",
        "    video_name = v.split('.')[0]\n",
        "        # Get src \n",
        "    video_path = os.path.join(file_dir, v)\n",
        "        # Get dest \n",
        "    save_path = os.path.join(save_dir, video_name+'.npy') \n",
        "        # Load and preprocess video\n",
        "    data = Video2Npy(file_path=video_path, resize=(224,224))\n",
        "    data = np.uint8(data)\n",
        "        # Save as .npy file\n",
        "    np.save(save_path, data)\n",
        "    \n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DQtDb08u-gy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "62f9c270-8f4d-4f02-c949-297308e4aaa1"
      },
      "source": [
        "RecentVideoSave2Npy(new_video_folder_dir,new_npy_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-112-f46c2779a406>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRecentVideoSave2Npy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_video_folder_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_npy_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'RecentVideoSave2Npy' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBYoXuGzDjR1"
      },
      "source": [
        "## Build Data Loader\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy4EMGZ9DCN3"
      },
      "source": [
        "from keras.utils import Sequence\n",
        "from keras.utils import np_utils\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    \"\"\"Data Generator inherited from keras.utils.Sequence\n",
        "    Args: \n",
        "        directory: the path of data set, and each sub-folder will be assigned to one class\n",
        "        batch_size: the number of data points in each batch\n",
        "        shuffle: whether to shuffle the data per epoch\n",
        "    Note:\n",
        "        If you want to load file with other data format, please fix the method of \"load_data\" as you want\n",
        "    \"\"\"\n",
        "    def __init__(self, directory, batch_size=1, shuffle=True, data_augmentation=True):\n",
        "        # Initialize the params\n",
        "        self.batch_size = batch_size\n",
        "        self.directory = directory\n",
        "        self.shuffle = shuffle\n",
        "        self.data_aug = data_augmentation\n",
        "        # Load all the save_path of files, and create a dictionary that save the pair of \"data:label\"\n",
        "        self.X_path, self.Y_dict = self.search_data() \n",
        "        # Print basic statistics information\n",
        "        self.print_stats()\n",
        "        return None\n",
        "        \n",
        "    def search_data(self):\n",
        "        X_path = []\n",
        "        Y_dict = {}\n",
        "        # list all kinds of sub-folders\n",
        "        self.dirs = sorted(os.listdir(self.directory))\n",
        "        one_hots = np_utils.to_categorical(range(len(self.dirs)))\n",
        "        for i,folder in enumerate(self.dirs):\n",
        "            folder_path = os.path.join(self.directory,folder)\n",
        "            for file in os.listdir(folder_path):\n",
        "                file_path = os.path.join(folder_path,file)\n",
        "                # append the each file path, and keep its label  \n",
        "                X_path.append(file_path)\n",
        "                Y_dict[file_path] = one_hots[i]\n",
        "        return X_path, Y_dict\n",
        "    \n",
        "    def print_stats(self):\n",
        "        # calculate basic information\n",
        "        self.n_files = len(self.X_path)\n",
        "        self.n_classes = len(self.dirs)\n",
        "        self.indexes = np.arange(len(self.X_path))\n",
        "        np.random.shuffle(self.indexes)\n",
        "        # Output states\n",
        "        print(\"Found {} files belonging to {} classes.\".format(self.n_files,self.n_classes))\n",
        "        for i,label in enumerate(self.dirs):\n",
        "            print('%10s : '%(label),i)\n",
        "        return None\n",
        "    \n",
        "    def __len__(self):\n",
        "        # calculate the iterations of each epoch\n",
        "        steps_per_epoch = np.ceil(len(self.X_path) / float(self.batch_size))\n",
        "        return int(steps_per_epoch)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Get the data of each batch\n",
        "        \"\"\"\n",
        "        # get the indexs of each batch\n",
        "        batch_indexs = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        # using batch_indexs to get path of current batch\n",
        "        batch_path = [self.X_path[k] for k in batch_indexs]\n",
        "        # get batch data\n",
        "        batch_x, batch_y = self.data_generation(batch_path)\n",
        "        return batch_x, batch_y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # shuffle the data at each end of epoch\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def data_generation(self, batch_path):\n",
        "        # load data into memory, you can change the np.load to any method you want\n",
        "        batch_x = [self.load_data(x) for x in batch_path]\n",
        "        batch_y = [self.Y_dict[x] for x in batch_path]\n",
        "        # transfer the data format and take one-hot coding for labels\n",
        "        batch_x = np.array(batch_x)\n",
        "        batch_y = np.array(batch_y)\n",
        "        return batch_x, batch_y\n",
        "      \n",
        "    def normalize(self, data):\n",
        "        mean = np.mean(data)\n",
        "        std = np.std(data)\n",
        "        return (data-mean) / std\n",
        "    \n",
        "    def random_flip(self, video, prob):\n",
        "        s = np.random.rand()\n",
        "        if s < prob:\n",
        "            video = np.flip(m=video, axis=2)\n",
        "        return video    \n",
        "    \n",
        "    def uniform_sampling(self, video, target_frames=64):\n",
        "        # get total frames of input video and calculate sampling interval \n",
        "        len_frames = int(len(video))\n",
        "        interval = int(np.ceil(len_frames/target_frames))\n",
        "        # init empty list for sampled video and \n",
        "        sampled_video = []\n",
        "        for i in range(0,len_frames,interval):\n",
        "            sampled_video.append(video[i])     \n",
        "        # calculate numer of padded frames and fix it \n",
        "        num_pad = target_frames - len(sampled_video)\n",
        "        padding = []\n",
        "        if num_pad>0:\n",
        "            for i in range(-num_pad,0):\n",
        "                try: \n",
        "                    padding.append(video[i])\n",
        "                except:\n",
        "                    padding.append(video[0])\n",
        "            sampled_video += padding     \n",
        "        # get sampled video\n",
        "        return np.array(sampled_video, dtype=np.float32)\n",
        "    \n",
        "    def random_clip(self, video, target_frames=64):\n",
        "        start_point = np.random.randint(len(video)-target_frames)\n",
        "        return video[start_point:start_point+target_frames]\n",
        "    \n",
        "    def dynamic_crop(self, video):\n",
        "        # extract layer of optical flow from video\n",
        "        opt_flows = video[...,3]\n",
        "        # sum of optical flow magnitude of individual frame\n",
        "        magnitude = np.sum(opt_flows, axis=0)\n",
        "        # filter slight noise by threshold \n",
        "        thresh = np.mean(magnitude)\n",
        "        magnitude[magnitude<thresh] = 0\n",
        "        # calculate center of gravity of magnitude map and adding 0.001 to avoid empty value\n",
        "        x_pdf = np.sum(magnitude, axis=1) + 0.001\n",
        "        y_pdf = np.sum(magnitude, axis=0) + 0.001\n",
        "        # normalize PDF of x and y so that the sum of probs = 1\n",
        "        x_pdf /= np.sum(x_pdf)\n",
        "        y_pdf /= np.sum(y_pdf)\n",
        "        # randomly choose some candidates for x and y \n",
        "        x_points = np.random.choice(a=np.arange(224), size=10, replace=True, p=x_pdf)\n",
        "        y_points = np.random.choice(a=np.arange(224), size=10, replace=True, p=y_pdf)\n",
        "        # get the mean of x and y coordinates for better robustness\n",
        "        x = int(np.mean(x_points))\n",
        "        y = int(np.mean(y_points))\n",
        "        # avoid to beyond boundaries of array\n",
        "        x = max(56,min(x,167))\n",
        "        y = max(56,min(y,167))\n",
        "        # get cropped video \n",
        "        return video[:,x-56:x+56,y-56:y+56,:]  \n",
        "    \n",
        "    def color_jitter(self,video):\n",
        "        # range of s-component: 0-1\n",
        "        # range of v component: 0-255\n",
        "        s_jitter = np.random.uniform(-0.2,0.2)\n",
        "        v_jitter = np.random.uniform(-30,30)\n",
        "        for i in range(len(video)):\n",
        "            hsv = cv2.cvtColor(video[i], cv2.COLOR_RGB2HSV)\n",
        "            s = hsv[...,1] + s_jitter\n",
        "            v = hsv[...,2] + v_jitter\n",
        "            s[s<0] = 0\n",
        "            s[s>1] = 1\n",
        "            v[v<0] = 0\n",
        "            v[v>255] = 255\n",
        "            hsv[...,1] = s\n",
        "            hsv[...,2] = v\n",
        "            video[i] = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
        "        return video\n",
        "        \n",
        "    def load_data(self, path):\n",
        "        # load the processed .npy files which have 5 channels (1-3 for RGB, 4-5 for optical flows)\n",
        "        data = np.load(path, mmap_mode='r')\n",
        "        data = np.float32(data)\n",
        "        # sampling 64 frames uniformly from the entire video\n",
        "        data = self.uniform_sampling(video=data, target_frames=64)\n",
        "        # whether to utilize the data augmentation\n",
        "        if  self.data_aug:\n",
        "            data[...,:3] = self.color_jitter(data[...,:3])\n",
        "            data = self.random_flip(data, prob=0.5)\n",
        "        # normalize rgb images and optical flows, respectively\n",
        "        data[...,:3] = self.normalize(data[...,:3])\n",
        "        data[...,3:] = self.normalize(data[...,3:])\n",
        "        return data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD9zlifLLbvV"
      },
      "source": [
        "num_epochs  = 30\n",
        "num_workers = 16\n",
        "batch_size  = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyHkH7BsQWa4"
      },
      "source": [
        "## TO DO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug7FgDohVoQy"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "dataset = 'ViolentFlow-opt'\n",
        "num_epochs  = 30\n",
        "num_workers = 16\n",
        "batch_size  = 8\n",
        "\n",
        "val_generator = DataGenerator(directory=new_npy_ParentFolder.format(dataset),\n",
        "                              batch_size=batch_size, \n",
        "                              data_augmentation=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_bKLbl8PhGw"
      },
      "source": [
        "predict=model.predict(val_generator)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UplOuSNgM1rS",
        "outputId": "040a0ac6-4344-4f30-d7a7-8dbfd49f42bd"
      },
      "source": [
        "predict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9359904, 0.0640096]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O88R_sCWR4hV",
        "outputId": "e66b3315-520a-42a3-acd7-7a4019e72735"
      },
      "source": [
        "predict.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omn7j8qV11jx"
      },
      "source": [
        "Fight = predict[0][0]\n",
        "Nonfight = predict[0][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tevL91DH2Ttg",
        "outputId": "ffab671f-0189-4e2c-88f9-ff310a7c2c2f"
      },
      "source": [
        "print(Fight)\n",
        "print(Nonfight)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9359904\n",
            "0.0640096\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "sggmmjLa3m_c",
        "outputId": "f7ec7db4-124a-4791-b21d-384863084199"
      },
      "source": [
        "x = ['Fight', 'Nonfight']\n",
        "y = [Fight, Nonfight]\n",
        "\n",
        "plt.bar(x, y)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvPBAfRUa6ai"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQ7wumeN1UFx"
      },
      "source": [
        "import cv2  \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os \n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zWSXwxh1RMa"
      },
      "source": [
        "## load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7wHAjQv1RMg"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFzP-Qm11RMh"
      },
      "source": [
        "from keras.models import load_model\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "model = load_model('/content/drive/MyDrive/4_19data/keras_model.h5',\n",
        "                  compile=False)\n",
        "model.compile(optimizer=sgd,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScDV5JCoa_GA",
        "outputId": "df62e636-1c50-4c7a-fed8-9c63b32ee4ee"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "dataset = 'ViolentFlow-opt'\n",
        "num_epochs  = 30\n",
        "num_workers = 16\n",
        "batch_size  = 8\n",
        "\n",
        "train_generator = DataGenerator(directory='/content/drive/MyDrive/4_19data/team_video_npy'.format(dataset),\n",
        "                              batch_size=batch_size, \n",
        "                              data_augmentation=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 53 files belonging to 2 classes.\n",
            "     Fight :  0\n",
            "  NonFight :  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO1eFHNCcGTp"
      },
      "source": [
        "pred = model.predict(train_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g08A1yIvhwa"
      },
      "source": [
        "# Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baCqRuhCwZRl",
        "outputId": "7be6abb8-1752-4392-e5e0-c518d55cc7ca"
      },
      "source": [
        "result = model.evaluate(train_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7/7 [==============================] - 44s 7s/step - loss: 0.4121 - accuracy: 0.8302\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTU3Skq8vhwa"
      },
      "source": [
        " # Print the Model Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-04-19T13:39:33.172520Z",
          "iopub.status.busy": "2021-04-19T13:39:33.172520Z",
          "iopub.status.idle": "2021-04-19T13:39:33.186482Z",
          "shell.execute_reply": "2021-04-19T13:39:33.186482Z",
          "shell.execute_reply.started": "2021-04-19T13:39:33.172520Z"
        },
        "tags": [],
        "id": "gAoZs9nUvhwb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4e14efb-220d-449a-89d0-f2092184fa3b"
      },
      "source": [
        "for name, value in zip(model.metrics_names, result):\n",
        "    print(name, value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 0.4121137261390686\n",
            "accuracy 0.8301886916160583\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}